{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WJ8trISTlYr-"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"RAG_Evaluation\")"
      ],
      "metadata": {
        "id": "UDB1aRk2rL_E"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "fluency_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "fluency_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRC4cRR9TH5E",
        "outputId": "80f4ba3c-9ba8-45a3-f43a-53b2a3a97d20"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = [\n",
        "    {\"id\": 1, \"text\": \"The Eiffel Tower is located in Paris.\"},\n",
        "    {\"id\": 2, \"text\": \"james cameroon directed titanic and avataar\"},\n",
        "    {\"id\": 3, \"text\": \"Python is a popular programming language for data science.\"},\n",
        "]"
      ],
      "metadata": {
        "id": "pQ2sJeZWTNFM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query):\n",
        "    query_embedding = retriever_model.encode(query)\n",
        "    context_scores = [\n",
        "        (doc, cosine_similarity([query_embedding], [retriever_model.encode(doc[\"text\"])])[0][0])\n",
        "        for doc in knowledge_base\n",
        "    ]\n",
        "    sorted_context = sorted(context_scores, key=lambda x: x[1], reverse=True)\n",
        "    return sorted_context[:1]"
      ],
      "metadata": {
        "id": "JiNLkEgWTR_s"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query, context):\n",
        "    prompt = f\"{query} Context: {' '.join([c[0]['text'] for c in context])}\"\n",
        "    response = generator(prompt, max_length=20)[0][\"generated_text\"]\n",
        "    return response"
      ],
      "metadata": {
        "id": "8lpZLqjUTSBx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_relevance(query, context):\n",
        "    return sum(cosine_similarity([retriever_model.encode(query)], [retriever_model.encode(doc[0][\"text\"])])[0][0] for doc in context) / len(context)\n"
      ],
      "metadata": {
        "id": "nIyTP6D_TSEJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_factual_consistency(response, context):\n",
        "    entailment_score = 0\n",
        "    for doc, _ in context:\n",
        "        inputs = nli_tokenizer.encode_plus(f\"{doc['text']}\", f\"{response}\", return_tensors=\"pt\")\n",
        "        outputs = nli_model(**inputs)\n",
        "        probs = outputs.logits.softmax(dim=1)\n",
        "        entailment_score += probs[0][2].item()\n",
        "    return entailment_score / len(context)\n"
      ],
      "metadata": {
        "id": "yZ1o_fD-TSG0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fluency(response):\n",
        "    inputs = fluency_tokenizer(response, return_tensors=\"pt\")\n",
        "    loss = fluency_model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "    perplexity = np.exp(loss.item())\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "f-Km-C0oTSIi"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_coverage(query, response):\n",
        "    inputs = nli_tokenizer.encode_plus(query, response, return_tensors=\"pt\")\n",
        "    outputs = nli_model(**inputs)\n",
        "    probs = outputs.logits.softmax(dim=1)\n",
        "    coverage_score = probs[0][2].item()\n",
        "    return coverage_score"
      ],
      "metadata": {
        "id": "LDy_s6BPTSK0"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mrr(retrieval_ranks):\n",
        "    reciprocal_ranks = [1/rank for rank in retrieval_ranks]\n",
        "    mrr = sum(reciprocal_ranks) / len(retrieval_ranks)\n",
        "    return mrr"
      ],
      "metadata": {
        "id": "nMnWJju8TSOu"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_map(retrieval_ranks):\n",
        "    precision_at_k = [(i+1) / rank for i, rank in enumerate(retrieval_ranks)]\n",
        "    map_score = sum(precision_at_k) / len(retrieval_ranks)\n",
        "    return map_score"
      ],
      "metadata": {
        "id": "eK3M2FBtTSRc"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval_accuracy(query, context):\n",
        "    query_embedding = retriever_model.encode(query)\n",
        "    all_doc_scores = [(doc, cosine_similarity([query_embedding], [retriever_model.encode(doc[\"text\"])])[0][0]) for doc in knowledge_base]\n",
        "    all_doc_scores = sorted(all_doc_scores, key=lambda x: x[1], reverse=True)\n",
        "    retrieval_ranks = []\n",
        "    for i, (doc, score) in enumerate(all_doc_scores):\n",
        "        if any(doc[\"id\"] == c[0][\"id\"] for c in context):\n",
        "            retrieval_ranks.append(i + 1)\n",
        "\n",
        "    mrr = calculate_mrr(retrieval_ranks)\n",
        "    map_score = calculate_map(retrieval_ranks)\n",
        "\n",
        "    return {\"MRR\": mrr, \"MAP\": map_score}"
      ],
      "metadata": {
        "id": "w3vZAlB7TSTy"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_evaluation(query):\n",
        "    context = retrieve_context(query)\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    relevance = evaluate_relevance(query, context)\n",
        "    factual_consistency = evaluate_factual_consistency(response, context)\n",
        "    fluency = evaluate_fluency(response)\n",
        "    coverage = evaluate_coverage(query, response)\n",
        "    retrieval_accuracy = evaluate_retrieval_accuracy(query, context)\n",
        "    logger.info(f\"Factual Consistency: {factual_consistency}\")\n",
        "    logger.info(f\"Fluency (Perplexity): {fluency}\")\n",
        "    logger.info(f\"Coverage: {coverage}\")\n",
        "    logger.info(f\"Retrieval Accuracy (MRR): {retrieval_accuracy['MRR']}\")\n",
        "    logger.info(f\"Retrieval Accuracy (MAP): {retrieval_accuracy['MAP']}\")\n",
        "    return {\n",
        "        \"response\": response,\n",
        "        \"relevance\": relevance,\n",
        "        \"factual_consistency\": factual_consistency,\n",
        "        \"fluency\": fluency,\n",
        "        \"coverage\": coverage,\n",
        "        \"retrieval_accuracy\": retrieval_accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "tz_Tyfy_Tqfi"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who directed titanic?\"\n",
        "results = rag_evaluation(query)\n",
        "\n",
        "\n",
        "print(\"RAG Evaluation Results:\", results)\n",
        "for metric, score in results.items():\n",
        "    print(f\"{metric.capitalize()}: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQULVIQ5Tqh4",
        "outputId": "c2062da8-3f01-470a-bc2a-639fa6772998"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Evaluation Results: {'response': \"who directed titanic? Context: james cameroon directed titanic and avataar's studio\", 'relevance': 0.6143321990966797, 'factual_consistency': 0.024692179635167122, 'fluency': 881.6091098568681, 'coverage': 0.0027736832853406668, 'retrieval_accuracy': {'MRR': 1.0, 'MAP': 1.0}}\n",
            "Response: who directed titanic? Context: james cameroon directed titanic and avataar's studio\n",
            "Relevance: 0.6143321990966797\n",
            "Factual_consistency: 0.024692179635167122\n",
            "Fluency: 881.6091098568681\n",
            "Coverage: 0.0027736832853406668\n",
            "Retrieval_accuracy: {'MRR': 1.0, 'MAP': 1.0}\n"
          ]
        }
      ]
    }
  ]
}